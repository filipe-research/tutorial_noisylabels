{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_sibgrapi20.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1ZrCCsqrud6C"
      ],
      "authorship_tag": "ABX9TyMOaTIJmV7WWDTa4S1sfpAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipe-research/tutorial_noisylabels/blob/main/codes/tutorial_sibgrapi20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaoig2tDG2Yx"
      },
      "source": [
        "# Tutorial: How to train your model when you cannot trust onthe annotations? - SIGBRAPI 2020\n",
        "\n",
        "Authors: Filipe Cordeiro, Gustavo Carneiro\\\n",
        "github: https://github.com/filipe-research/tutorial_noisylabels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WWHNBr8tHRr"
      },
      "source": [
        "DATASET: \n",
        "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
        "\n",
        "[Read more.](https://www.kaggle.com/c/digit-recognizer)\n",
        "\n",
        "\n",
        "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN4wNtYaLZGE"
      },
      "source": [
        "# Setup\n",
        "\n",
        "import libraries, hyperparameters and main methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRJuYj3TXt6F",
        "cellView": "both"
      },
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi2xjx4NZpDu"
      },
      "source": [
        "#@title Define Hyperparameters\n",
        "\n",
        "training_size= 2000\n",
        "num_epochs = 50 # number of times which the entire dataset is passed throughout the model\n",
        "batch_size = 128 # the size of input data took for one iteration\n",
        "lr = 1e-3 # size of step\n",
        "seed=123\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXWpKnM1YNs0",
        "cellView": "both"
      },
      "source": [
        "#@title Downloading MNIST data\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                       ]), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                       ]))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA5FjjwlYope"
      },
      "source": [
        "# Reduce training size  (just for fast training during the tutorial)\n",
        "train_data.data = train_data.data[:training_size]\n",
        "train_data.targets = train_data.targets[:training_size]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xkO6mssbKiE"
      },
      "source": [
        "#generate dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True,num_workers=4,pin_memory=True)\n",
        "\n",
        "eval_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = False,num_workers=4,pin_memory=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False,num_workers=4)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09htCjN-cF-P"
      },
      "source": [
        "#plot images of dataset\n",
        "\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "for i in range(15):\n",
        "  plt.subplot(3,5,i+1)\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NYQEP2WyYKf"
      },
      "source": [
        "#@title CNN Arquitecture\n",
        "#CNN Architecture\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 30)\n",
        "        self.fc2 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return(x)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23sUxKZCa6wF"
      },
      "source": [
        "ce_loss = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv0NEXqHGh2H"
      },
      "source": [
        "#@title test method\n",
        "def test(net, loss_function, loader):\n",
        "  net.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for data, target in loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = net(data)\n",
        "      test_loss += loss_function(output, target)\n",
        "      _, pred = torch.max(output, 1)  \n",
        "      correct += pred.eq(target).cpu().sum().item() \n",
        "  acc = 100.*correct/len(loader.dataset)\n",
        "  test_loss /= len(loader)\n",
        "  \n",
        "  return acc, test_loss"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEOyCUwzp9mk"
      },
      "source": [
        "#@title evaluate method\n",
        "\n",
        "def eval(net, loader):\n",
        "  net.eval()\n",
        "  \n",
        "  losses = torch.zeros(len(loader.dataset))    \n",
        "  CE = nn.CrossEntropyLoss(reduction='none')\n",
        "  with torch.no_grad():\n",
        "    for i ,(images,labels) in enumerate(loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        loss = CE(outputs, labels)\n",
        "        for b in range(images.size(0)):\n",
        "          idx = batch_size*i+b\n",
        "          losses[idx]=loss[b] \n",
        "  \n",
        "  losses = (losses-losses.min())/(losses.max()-losses.min())    \n",
        "  input_loss = losses.reshape(-1,1)\n",
        "\n",
        "  # fit a two-component GMM to the loss\n",
        "  gmm = GaussianMixture(n_components=2,max_iter=50,tol=1e-2,reg_covar=5e-4)\n",
        "  gmm.fit(input_loss)\n",
        "  prob = gmm.predict_proba(input_loss) \n",
        "  prob = prob[:,gmm.means_.argmin()] \n",
        "  return losses, prob\n",
        "\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBKt_Y9JAZHf"
      },
      "source": [
        "def train_epoch(net, loss_function, dataloader, optimizer):\n",
        "  train_loss = train_acc = correct = 0\n",
        "  net.train()\n",
        "  for i ,(images,labels) in enumerate(dataloader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    train_loss+= loss\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    _, pred = torch.max(outputs, 1)  \n",
        "    correct += pred.eq(labels).cpu().sum().item() \n",
        "        \n",
        "  train_loss/=len(dataloader)\n",
        "  train_acc=100.*correct/len(dataloader.dataset)\n",
        "  return train_loss, train_acc"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Ex9GknBjsQ"
      },
      "source": [
        "def train_test_model(loss_function):\n",
        "  torch.manual_seed(seed)\n",
        "  net = Net()\n",
        "  net = net.to(device)\n",
        "  optimizer = torch.optim.Adam( net.parameters(), lr=lr) \n",
        "\n",
        "  logs={'train_loss':[],'train_acc':[], 'test_loss':[], 'test_acc':[],'eval_loss':[]}\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(net, loss_function, train_loader, optimizer)\n",
        "    \n",
        "    logs['train_loss'].append(train_loss)\n",
        "    logs['train_acc'].append(train_acc)\n",
        "\n",
        "    eval_loss, _ = eval(net, eval_loader)\n",
        "    logs['eval_loss'].append(eval_loss)\n",
        "    \n",
        "    acc_test, loss_test=test(net, loss_function, test_loader)\n",
        "    logs['test_loss'].append(loss_test)\n",
        "    logs['test_acc'].append(acc_test)\n",
        "\n",
        "    print('Epoch [{}/{}],\\ttrain loss:{:.4f},\\ttrain acc:{:.2f},\\ttest loss:{:.4f},\\ttest accuracy:{:.2f}'.format(epoch,num_epochs,train_loss,train_acc, loss_test, acc_test))\n",
        "  return logs\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xTsEh_QLMA5"
      },
      "source": [
        "# Run Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Huta4yh6Fv9"
      },
      "source": [
        "#run baseline\n",
        "logs_baseline = train_test_model(ce_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBQ6MzVSpYr5"
      },
      "source": [
        "fig = plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(logs_baseline['train_acc'], label='train', linewidth=2)\n",
        "plt.plot(logs_baseline['test_acc'], label='test', linewidth=2)\n",
        "plt.legend(frameon=False)\n",
        "plt.grid(True, color=\"#93a1a1\", alpha=0.3)\n",
        "plt.ylabel(\"Accuracy\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.xlabel(\"Epoch\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(logs_baseline['train_loss'], label='Training loss', linewidth=2)\n",
        "plt.plot(logs_baseline['test_loss'], label='Validation loss', linewidth=2)\n",
        "plt.legend(frameon=False)\n",
        "# plt.grid()\n",
        "plt.grid(True, color=\"#93a1a1\", alpha=0.3)\n",
        "plt.xlabel(\"Epoch\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.ylabel(\"Loss\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUFPyP9itC8M"
      },
      "source": [
        "# Generate Noisy MNIST\n",
        "\n",
        "Build synthetic noise benchmark on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_yPsPpnq8iQ"
      },
      "source": [
        "gt_labels = train_data.targets.clone()   #ground truth labels (clean)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghufl14KazUw"
      },
      "source": [
        "##generate noise\n",
        "\n",
        "transition = {0:0,2:0,4:7,7:7,1:1,9:1,3:5,5:3,6:6,8:8} # class transition for asymmetric noise\n",
        "\n",
        "def generate_noise(nr, gt_labels, noise_mode='sym'):\n",
        "  noise_label = []\n",
        "  idx = list(range(len(gt_labels)))\n",
        "  random.shuffle(idx)\n",
        "  num_noise = int(nr*len(gt_labels))            \n",
        "  noise_idx = idx[:num_noise]\n",
        "  for i in range(len(gt_labels)):\n",
        "      if i in noise_idx:\n",
        "        if noise_mode=='sym':\n",
        "          noiselabel = random.randint(0,9)  \n",
        "          noise_label.append(noiselabel)\n",
        "        elif noise_mode=='asym':   \n",
        "          noiselabel = transition[gt_labels[i].item()]\n",
        "          noise_label.append(noiselabel)                    \n",
        "      else:    \n",
        "        noise_label.append(gt_labels[i])  \n",
        "  return torch.tensor(noise_label)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWptIUJMMlug"
      },
      "source": [
        "#generate noisy_benchmark\n",
        "label_noise={}\n",
        "label_noise['50_sym']=generate_noise(0.5, gt_labels, 'sym')\n",
        "label_noise['70_sym']=generate_noise(0.7, gt_labels, 'sym')\n",
        "label_noise['40_asym']=generate_noise(0.4, gt_labels, 'asym')"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ3vbE0ofak5"
      },
      "source": [
        "print('real noise rate:{:.2f}'.format((np.array(label_noise['70_sym'])!=np.array(gt_labels)).sum()/len(gt_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsWRLivRYqZB"
      },
      "source": [
        "#plot images of dataset\n",
        "\n",
        "# train_data.targets = label_noise['70_sym']  #update current labels\n",
        "train_loader.dataset.targets = label_noise['70_sym'].clone()\n",
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "for i in range(15):\n",
        "  plt.subplot(3,5,i+1)\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"label: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIfWVsPrXwyQ"
      },
      "source": [
        "noisy_map = np.zeros((10, 10))\n",
        "nl = label_noise['70_sym']\n",
        "for i in range(len(gt_labels)):\n",
        "  noisy_map[int(nl[i])][int(gt_labels[i])] += 1\n",
        "\n",
        "#calculate de percentage. Divides each element by the sum of the row\n",
        "percent_map = noisy_map/noisy_map.sum(axis=0, keepdims=True)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxflo7QsY19C"
      },
      "source": [
        "df_cm = pd.DataFrame(percent_map, index = [0,1,2,3,4,5,6,7,8,9],\n",
        "                  columns = list(range(10)))\n",
        "plt.figure(figsize = (8,6))\n",
        "\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.ylabel('noisy labels')\n",
        "plt.xlabel('true labels')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd96r-G9aAC0"
      },
      "source": [
        "unique_elements, counts_elements = np.unique(nl, return_counts=True)\n",
        "# unique_elements, counts_elements = np.unique(gt_labels, return_counts=True)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNee3-FaaDKn"
      },
      "source": [
        "plt.bar(unique_elements,counts_elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ywe_lYxRSgk"
      },
      "source": [
        "## Train noisy baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j03rY9Uh8AeZ"
      },
      "source": [
        "#train with 50% noise rate\n",
        "train_loader.dataset.targets = label_noise['50_sym'].clone()  #update current labels\n",
        "logs_nb_50_sym = train_test_model(ce_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7IF70IHSr-A"
      },
      "source": [
        "#train with 50% noise rate\n",
        "train_loader.dataset.targets = label_noise['70_sym'].clone()  #update current labels\n",
        "logs_nb_70_sym = train_test_model(ce_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfTqFGtq90KH"
      },
      "source": [
        "#train with 40% asym. noise rate\n",
        "train_loader.dataset.targets = label_noise['40_asym'].clone()  #update current labels\n",
        "logs_nb_40_asym = train_test_model(ce_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MBgZzr0UurG"
      },
      "source": [
        "fig = plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(logs_baseline['train_acc'], label='train_clean', linewidth=2, linestyle='--',color='k')\n",
        "plt.plot(logs_nb_50_sym['train_acc'], label='train_50_sym', linewidth=2, linestyle='--', color='r')\n",
        "plt.plot(logs_nb_70_sym['train_acc'], label='train_70_sym', linewidth=2, linestyle='--', color='g')\n",
        "plt.plot(logs_nb_40_asym['train_acc'], label='train_40_asym', linewidth=2, linestyle='--', color='b')\n",
        "plt.plot(logs_baseline['test_acc'], label='test_clean', linewidth=2, color='k')\n",
        "plt.plot(logs_nb_50_sym['test_acc'], label='test_50_sym', linewidth=2, color='r')\n",
        "plt.plot(logs_nb_70_sym['test_acc'], label='test_70_sym', linewidth=2, color='g')\n",
        "plt.plot(logs_nb_40_asym['test_acc'], label='test_40_asym', linewidth=2, color='b')\n",
        "# plt.legend(frameon=False)\n",
        "plt.grid(True, color=\"#93a1a1\", alpha=0.3)\n",
        "plt.ylabel(\"Accuracy\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.xlabel(\"Epoch\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
        "          fancybox=True, shadow=True, ncol=4)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(logs_baseline['train_loss'], label='train_clean', linewidth=2, linestyle='--',color='k')\n",
        "plt.plot(logs_nb_50_sym['train_loss'], label='train_50_sym', linewidth=2, linestyle='--', color='r')\n",
        "plt.plot(logs_nb_70_sym['train_loss'], label='train_70_sym', linewidth=2, linestyle='--', color='g')\n",
        "plt.plot(logs_nb_40_asym['train_loss'], label='train_40_asym', linewidth=2, linestyle='--', color='b')\n",
        "plt.plot(logs_baseline['test_loss'], label='test_clean', linewidth=2, color='k')\n",
        "plt.plot(logs_nb_50_sym['test_loss'], label='test_50_sym', linewidth=2, color='r')\n",
        "plt.plot(logs_nb_70_sym['test_loss'], label='test_70_sym', linewidth=2, color='g')\n",
        "plt.plot(logs_nb_40_asym['test_loss'], label='test_40_asym', linewidth=2, color='b')\n",
        "# plt.legend(frameon=False)\n",
        "plt.grid(True, color=\"#93a1a1\", alpha=0.3)\n",
        "plt.xlabel(\"Epoch\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.ylabel(\"Loss\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8aeLfzpgAEY"
      },
      "source": [
        "# Combating Noisy Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7CBu1yggRMk"
      },
      "source": [
        "## Robust Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ada_gdE9r7m"
      },
      "source": [
        "#Active Passive Loss (ICML 2020)\n",
        "#paper code: https://github.com/HanxunH/Active-Passive-Losses"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-KbXXgNAWME"
      },
      "source": [
        "class NormalizedCrossEntropy(torch.nn.Module):\n",
        "    def __init__(self, num_classes=10, scale=1.0):\n",
        "        super(NormalizedCrossEntropy, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, pred, labels):\n",
        "        pred = F.log_softmax(pred, dim=1)\n",
        "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float()\n",
        "        nce = -1 * torch.sum(label_one_hot * pred, dim=1) / (- pred.sum(dim=1))\n",
        "        return self.scale * nce.mean()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WTiQi-AluW"
      },
      "source": [
        "class ReverseCrossEntropy(torch.nn.Module):\n",
        "    def __init__(self, num_classes, scale=1.0):\n",
        "        super(ReverseCrossEntropy, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, pred, labels):\n",
        "        pred = F.softmax(pred, dim=1)\n",
        "        pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
        "        label_one_hot = torch.clamp(label_one_hot, min=1e-4, max=1.0)\n",
        "        rce = (-1*torch.sum(pred * torch.log(label_one_hot), dim=1))\n",
        "        return self.scale * rce.mean()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6FD_S0ABiN"
      },
      "source": [
        "class NCEandRCE(torch.nn.Module):\n",
        "    def __init__(self, alpha=0.1, beta=1, num_classes=10):\n",
        "        super(NCEandRCE, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.nce = NormalizedCrossEntropy(scale=alpha, num_classes=num_classes)\n",
        "        self.rce = ReverseCrossEntropy(scale=beta, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, pred, labels):\n",
        "        return self.nce(pred, labels) + self.rce(pred, labels)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D62rjSyTZlAR"
      },
      "source": [
        "class NormalizedFocalLoss(torch.nn.Module):\n",
        "    def __init__(self, scale=1.0, gamma=0, num_classes=10, alpha=None, size_average=True):\n",
        "        super(NormalizedFocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.size_average = size_average\n",
        "        self.num_classes = num_classes\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        target = target.view(-1, 1)\n",
        "        logpt = F.log_softmax(input, dim=1)\n",
        "        normalizor = torch.sum(-1 * (1 - logpt.data.exp()) ** self.gamma * logpt, dim=1)\n",
        "        logpt = logpt.gather(1, target)\n",
        "        logpt = logpt.view(-1)\n",
        "        pt = torch.autograd.Variable(logpt.data.exp())\n",
        "        loss = -1 * (1-pt)**self.gamma * logpt\n",
        "        loss = self.scale * loss / normalizor\n",
        "\n",
        "        if self.size_average:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss.sum()"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBWZBo7MZbCf"
      },
      "source": [
        "class NFLandRCE(torch.nn.Module):\n",
        "    def __init__(self, alpha, beta, num_classes=10, gamma=0.5):\n",
        "        super(NFLandRCE, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.nfl = NormalizedFocalLoss(scale=alpha, gamma=gamma, num_classes=num_classes)\n",
        "        self.rce = ReverseCrossEntropy(scale=beta, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, pred, labels):\n",
        "        return self.nfl(pred, labels) + self.rce(pred, labels)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkon_NkxdN6D"
      },
      "source": [
        "class MeanAbsoluteError(torch.nn.Module):\n",
        "    def __init__(self, num_classes, scale=1.0):\n",
        "        super(MeanAbsoluteError, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.scale = scale\n",
        "        return\n",
        "\n",
        "    def forward(self, pred, labels):\n",
        "        pred = F.softmax(pred, dim=1)\n",
        "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
        "        mae = 1. - torch.sum(label_one_hot * pred, dim=1)\n",
        "        # Note: Reduced MAE\n",
        "        # Original: torch.abs(pred - label_one_hot).sum(dim=1)\n",
        "        # $MAE = \\sum_{k=1}^{K} |\\bm{p}(k|\\bm{x}) - \\bm{q}(k|\\bm{x})|$\n",
        "        # $MAE = \\sum_{k=1}^{K}\\bm{p}(k|\\bm{x}) - p(y|\\bm{x}) + (1 - p(y|\\bm{x}))$\n",
        "        # $MAE = 2 - 2p(y|\\bm{x})$\n",
        "        #\n",
        "        return self.scale * mae.mean()"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVodnr2wdBep"
      },
      "source": [
        "class NCEandMAE(torch.nn.Module):\n",
        "    def __init__(self, alpha, beta, num_classes=10):\n",
        "        super(NCEandMAE, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.nce = NormalizedCrossEntropy(scale=alpha, num_classes=num_classes)\n",
        "        self.mae = MeanAbsoluteError(scale=beta, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, pred, labels):\n",
        "        return self.nce(pred, labels) + self.mae(pred, labels)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VUu_4tTljdH"
      },
      "source": [
        "### Train with APL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ChbOSlCgWIm"
      },
      "source": [
        "#train with 70% noise rate\n",
        "train_loader.dataset.targets = label_noise['70_sym'].clone()  #update current labels\n",
        "criterion =NCEandMAE(alpha=1, beta=1)\n",
        "logs_apl_70_sym = train_test_model(criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtG5LMy7l259"
      },
      "source": [
        "fig = plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(logs_baseline['test_acc'], label='test_baseline_clean', linewidth=2, color='k')\n",
        "plt.plot(logs_nb_70_sym['test_acc'], label='test_baseline_70_sym', linewidth=2, color='r')\n",
        "plt.plot(logs_apl_70_sym['test_acc'], label='test__APL_70_sym', linewidth=2, color='b')\n",
        "# plt.plot(logs_nb_40_asym['test_acc'], label='test_40_asym', linewidth=2, color='b')\n",
        "plt.grid(True, color=\"#93a1a1\", alpha=0.3)\n",
        "plt.ylabel(\"Accuracy\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.xlabel(\"Epoch\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
        "          fancybox=True, shadow=True, ncol=4)\n",
        "plt.ylim(70, 100,10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0iU1KNwgg9c"
      },
      "source": [
        "### small-loss trick"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxhqpwY50z8k"
      },
      "source": [
        "# noisy_labels = label_noise['70_sym']\n",
        "nl = label_noise['70_sym']\n",
        "inds_noisy = np.asarray([ind for ind in range(len(nl)) if nl[ind] != gt_labels[ind]])\n",
        "inds_clean = np.delete(np.arange(len(nl)), inds_noisy)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nijAOl4I15lU"
      },
      "source": [
        "plt.hist(logs_nb_70_sym['eval_loss'][-1][inds_noisy],label='noisy',bins=100,alpha=0.5)\n",
        "plt.hist(logs_nb_70_sym['eval_loss'][-1][inds_clean],label='clean',bins=100,alpha=0.5)\n",
        "plt.xlabel(\"Normalized loss\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkXsjASH3pbL"
      },
      "source": [
        "avg_loss_clean = [logs_nb_70_sym['eval_loss'][i][inds_clean].mean() for i in range(num_epochs)]\n",
        "avg_loss_noisy = [logs_nb_70_sym['eval_loss'][i][inds_noisy].mean() for i in range(num_epochs)]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVKKO4YpgvBA"
      },
      "source": [
        "plt.plot(avg_loss_clean, label='clean',linewidth=2)\n",
        "plt.plot(avg_loss_noisy, label='noisy',linewidth=2)\n",
        "plt.ylabel(\"Avg. Loss\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.xlabel(\"Epoch\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEaN8JhW4bCM"
      },
      "source": [
        "## Remove Noisy Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrKec-KO4mik"
      },
      "source": [
        "def train_test_filtering(loss_function,trainloader):\n",
        "  torch.manual_seed(seed)\n",
        "  net = Net()\n",
        "  net = net.to(device)\n",
        "  optimizer = torch.optim.Adam( net.parameters(), lr=lr) \n",
        "\n",
        "  logs={'train_loss':[],'train_acc':[], 'test_loss':[], 'test_acc':[],'eval_loss':[]}\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(net, loss_function, trainloader, optimizer)\n",
        "    \n",
        "    logs['train_loss'].append(train_loss)\n",
        "    logs['train_acc'].append(train_acc)\n",
        "    eval_loss, prob = eval(net, eval_loader)\n",
        "    logs['eval_loss'].append(eval_loss)\n",
        "    \n",
        "    acc_test, loss_test=test(net, loss_function, test_loader)\n",
        "    logs['test_loss'].append(loss_test)\n",
        "    logs['test_acc'].append(acc_test)\n",
        "\n",
        "    if epoch>10:\n",
        "      pred_idx_clean = (prob > 0.5).nonzero()[0]\n",
        "      \n",
        "      train_clean = Subset(train_data, pred_idx_clean)    #get the subset of samples predicted as clean\n",
        "      clean_loader = torch.utils.data.DataLoader(train_clean,\n",
        "                                              batch_size = batch_size,\n",
        "                                              shuffle = True,num_workers=4,pin_memory=True)\n",
        "      trainloader=clean_loader\n",
        "\n",
        "    print('Epoch [{}/{}],\\ttrain loss:{:.4f},\\ttrain acc:{:.2f},\\ttest loss:{:.4f},\\ttest accuracy:{:.2f}'.format(epoch,num_epochs,train_loss,train_acc, loss_test, acc_test))\n",
        "  \n",
        "  return logs"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmnupS_26Xj_"
      },
      "source": [
        "#train with 70% sym noise\n",
        "train_loader.dataset.targets = label_noise['70_sym'].clone()  #update current labels\n",
        "logs_remove_70_sym = train_test_filtering(ce_loss,train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCVSst_oCJVV"
      },
      "source": [
        "fig = plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(logs_baseline['test_acc'], label='test_baseline_clean', linewidth=2, color='k')\n",
        "plt.plot(logs_nb_70_sym['test_acc'], label='test_baseline_70_sym', linewidth=2, color='r')\n",
        "plt.plot(logs_apl_70_sym['test_acc'], label='test__APL_70_sym', linewidth=2, color='b')\n",
        "plt.plot(logs_remove_70_sym['test_acc'], label='test_remove_70_sym', linewidth=2, color='g')\n",
        "plt.grid(True, color=\"#93a1a1\", alpha=0.3)\n",
        "plt.ylabel(\"Accuracy\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.xlabel(\"Epoch\", labelpad=15, fontsize=12, color=\"#333533\", fontweight='bold');\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
        "          fancybox=True, shadow=True, ncol=4)\n",
        "plt.ylim(70, 100,10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}